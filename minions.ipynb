{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: PEFT is not installed. Please install it with `pip install peft`.\n",
      "Warning: cartesia_mlx is not installed. If you want to use cartesia_mlx, please follow the instructions in the README to install it.\n",
      "Warning: huggingface inference client is not installed. If you want to use huggingface inference client, please install it with `pip install huggingface-hub`\n",
      "✅ NVIDIA attestation SDK loaded successfully\n",
      "chromadb is not installed. Please install it using `pip install chromadb`.\n",
      "SentenceTransformer not installed\n",
      "faiss not installed\n"
     ]
    }
   ],
   "source": [
    "# Misc imports\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Import Minions + Minions\n",
    "from minions.minions import Minions\n",
    "from minions.minion import Minion\n",
    "\n",
    "# Import Minion Clients\n",
    "from minions.clients.ollama import OllamaClient\n",
    "from minions.clients.tokasaurus import TokasaurusClient\n",
    "from minions.clients.openai import OpenAIClient\n",
    "from minions.clients.anthropic import AnthropicClient\n",
    "from minions.clients.together import TogetherClient\n",
    "\n",
    "# Import Pydantic\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Clients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Specify structured output schema for the OllamaClient. Run this block as is! Do _NOT_ make any modifications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StructuredLocalOutput(BaseModel):\n",
    "    explanation: str\n",
    "    citation: str | None\n",
    "    answer: str | None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Instantiate Clients: Here we instantiate our local client to be ollama and remote client to be OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "import os\n",
    "import openai\n",
    "import requests\n",
    "\n",
    "from minions.usage import Usage\n",
    "from minions.clients.base import MinionsClient\n",
    "\n",
    "class OpenAIClient(MinionsClient):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"gpt-4o\",\n",
    "        api_key: Optional[str] = None,\n",
    "        temperature: float = 0.0,\n",
    "        max_tokens: int = 4096,\n",
    "        base_url: Optional[str] = None,\n",
    "        use_responses_api: bool = False,\n",
    "        local: bool = False,\n",
    "        tools: List[Dict[str, Any]] = None,\n",
    "        reasoning_effort: str = \"low\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the OpenAI client.\n",
    "\n",
    "        Args:\n",
    "            model_name: The name of the model to use (default: \"gpt-4o\")\n",
    "            api_key: OpenAI API key (optional, falls back to environment variable if not provided)\n",
    "            temperature: Sampling temperature (default: 0.0)\n",
    "            max_tokens: Maximum number of tokens to generate (default: 4096)\n",
    "            base_url: Base URL for the OpenAI API (optional, falls back to OPENAI_BASE_URL environment variable or default URL)\n",
    "            use_responses_api: Whether to use responses API for o1-pro models (default: False)\n",
    "            tools: List of tools for function calling (default: None)\n",
    "            reasoning_effort: Reasoning effort level for o1 models (default: \"low\")\n",
    "            local: If this is communicating with a local client (default: False)\n",
    "            **kwargs: Additional parameters passed to base class\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            model_name=model_name,\n",
    "            api_key=api_key,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            base_url=base_url,\n",
    "            local=local,\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        # Client-specific configuration\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        self.api_key = api_key or os.getenv(\"OPENAI_API_KEY\")\n",
    "        self.base_url = base_url or os.getenv(\n",
    "            \"OPENAI_BASE_URL\", \"https://api.openai.com/v1\"\n",
    "        )\n",
    "\n",
    "        # Initialize the client\n",
    "        self.client = openai.OpenAI(api_key=self.api_key, base_url=self.base_url)\n",
    "        if \"o1-pro\" in self.model_name:\n",
    "            self.use_responses_api = True\n",
    "        else:\n",
    "            self.use_responses_api = use_responses_api\n",
    "        self.tools = tools\n",
    "        self.reasoning_effort = reasoning_effort\n",
    "\n",
    "        # If we are using a local client, we want to check to see if the\n",
    "        # local server is running or not\n",
    "        if self.local:\n",
    "            try:\n",
    "                self.check_local_server_health()\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                raise RuntimeError((\"Local OpenAI server at {} is \"\n",
    "                    \"not running or reachable.\".format(self.base_url)))\n",
    "\n",
    "    def responses(\n",
    "        self, messages: List[Dict[str, Any]], **kwargs\n",
    "    ) -> Tuple[List[str], Usage]:\n",
    "\n",
    "        assert len(messages) > 0, \"Messages cannot be empty.\"\n",
    "\n",
    "        if \"response_format\" in kwargs:\n",
    "            # handle new format of structure outputs from openai\n",
    "            kwargs[\"text\"] = {\"format\": kwargs[\"response_format\"]}\n",
    "            del kwargs[\"response_format\"]\n",
    "            if self.tools:\n",
    "                del kwargs[\"text\"]\n",
    "\n",
    "        try:\n",
    "\n",
    "            # replace an messages that have \"system\" with \"developer\"\n",
    "            for message in messages:\n",
    "                if message[\"role\"] == \"system\":\n",
    "                    message[\"role\"] = \"developer\"\n",
    "\n",
    "            params = {\n",
    "                \"model\": self.model_name,\n",
    "                \"input\": messages,\n",
    "                \"max_output_tokens\": self.max_tokens,\n",
    "                \"tools\": self.tools,\n",
    "                **kwargs,\n",
    "            }\n",
    "            if \"o1\" in self.model_name or \"o3\" in self.model_name:\n",
    "                params[\"reasoning\"] = {\"effort\": self.reasoning_effort}\n",
    "                # delete \"tools\" from params\n",
    "                del params[\"tools\"]\n",
    "\n",
    "            response = self.client.responses.create(\n",
    "                **params,\n",
    "            )\n",
    "            output_text = response.output\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error during OpenAI API call: {e}\")\n",
    "            raise\n",
    "\n",
    "        outputs = [output_text[1].content[0].text]\n",
    "\n",
    "        # Extract usage information if it exists\n",
    "        if response.usage is None:\n",
    "            usage = Usage(prompt_tokens=0, completion_tokens=0)\n",
    "        else:\n",
    "            usage = Usage(\n",
    "                prompt_tokens=response.usage.input_tokens,\n",
    "                completion_tokens=response.usage.output_tokens,\n",
    "            )\n",
    "\n",
    "        return outputs, usage\n",
    "\n",
    "    def chat(self, messages: List[Dict[str, Any]], **kwargs) -> Tuple[List[str], Usage]:\n",
    "        \"\"\"\n",
    "        Handle chat completions using the OpenAI API.\n",
    "\n",
    "        Args:\n",
    "            messages: List of message dictionaries with 'role' and 'content' keys\n",
    "            **kwargs: Additional arguments to pass to openai.chat.completions.create\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (List[str], Usage) containing response strings and token usage\n",
    "        \"\"\"\n",
    "        if self.use_responses_api:\n",
    "            return self.responses(messages, **kwargs)\n",
    "        else:\n",
    "            assert len(messages) > 0, \"Messages cannot be empty.\"\n",
    "\n",
    "            try:\n",
    "                params = {\n",
    "                    \"model\": self.model_name,\n",
    "                    \"messages\": messages,\n",
    "                    \"max_completion_tokens\": self.max_tokens,\n",
    "                    **kwargs,\n",
    "                }\n",
    "\n",
    "                # Only add temperature if NOT using the reasoning models (e.g., o3-mini model)\n",
    "                if \"o1\" not in self.model_name and \"o3\" not in self.model_name:\n",
    "                    params[\"temperature\"] = self.temperature\n",
    "                if \"o1\" in self.model_name or \"o3\" in self.model_name:\n",
    "                    params[\"reasoning_effort\"] = self.reasoning_effort\n",
    "\n",
    "                response = self.client.chat.completions.create(**params)\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error during OpenAI API call: {e}\")\n",
    "                raise\n",
    "\n",
    "            # Extract usage information if it exists\n",
    "            if response.usage is None:\n",
    "                usage = Usage(prompt_tokens=0, completion_tokens=0)\n",
    "            else:\n",
    "                usage = Usage(\n",
    "                    prompt_tokens=response.usage.prompt_tokens,\n",
    "                    completion_tokens=response.usage.completion_tokens,\n",
    "                )\n",
    "\n",
    "            # The content is now nested under message\n",
    "            if self.local:\n",
    "                return [choice.message.content for choice in response.choices], usage, [choice.finish_reason for choice in response.choices]\n",
    "            else:\n",
    "                return [choice.message.content for choice in response.choices], usage\n",
    "\n",
    "    def check_local_server_health(self):\n",
    "        \"\"\"\n",
    "        If we are using a local client, we want to be able\n",
    "        to check if the local server is running or not\n",
    "        \"\"\"\n",
    "        resp = requests.get(f\"{self.base_url}/health\")\n",
    "        resp.raise_for_status()\n",
    "        return resp.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "from pydantic import BaseModel\n",
    "from typing import Any, Dict, List, Optional, Union, Tuple\n",
    "import json\n",
    "import re\n",
    "\n",
    "from minions.usage import Usage\n",
    "from minions.clients.base import MinionsClient\n",
    "\n",
    "class OllamaClient(MinionsClient):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"llama-3.2\",\n",
    "        temperature: float = 0.0,\n",
    "        max_tokens: int = 2048,\n",
    "        num_ctx: int = 48000,\n",
    "        structured_output_schema: Optional[BaseModel] = None,\n",
    "        use_async: bool = False,\n",
    "        tool_calling: bool = False,\n",
    "        thinking: bool = False,\n",
    "        mcp_client=None,\n",
    "        max_tool_iterations: int = 5,\n",
    "        local: bool = True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Initialize Ollama Client.\n",
    "        \n",
    "        Args:\n",
    "            model_name: The Ollama model to use (default: \"llama-3.2\")\n",
    "            temperature: Sampling temperature (default: 0.0)\n",
    "            max_tokens: Maximum number of tokens to generate (default: 2048)\n",
    "            num_ctx: Context window size (default: 48000)\n",
    "            structured_output_schema: Optional Pydantic model for structured output\n",
    "            use_async: Whether to use async API calls (default: False)\n",
    "            tool_calling: Whether to support tool calling (default: False)\n",
    "            thinking: Whether to enable thinking mode (default: False)\n",
    "            mcp_client: Optional MCP client for tool calling (SyncMCPClient)\n",
    "            max_tool_iterations: Maximum number of tool calling iterations (default: 5)\n",
    "            **kwargs: Additional parameters passed to base class\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            model_name=model_name,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            local=local,\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        # Client-specific configuration\n",
    "        self.num_ctx = num_ctx\n",
    "\n",
    "        if self.model_name == \"granite3.2-vision\":\n",
    "            self.num_ctx = 131072\n",
    "            self.max_tokens = 131072\n",
    "\n",
    "        self.use_async = use_async\n",
    "        self.return_tools = tool_calling\n",
    "        self.thinking = thinking\n",
    "        \n",
    "        # MCP tooling configuration\n",
    "        self.mcp_client = mcp_client\n",
    "        self.max_tool_iterations = max_tool_iterations\n",
    "        self.mcp_tools_enabled = mcp_client is not None\n",
    "\n",
    "        # If we want structured schema output:\n",
    "        self.format_structured_output = None\n",
    "        if structured_output_schema:\n",
    "            self.format_structured_output = structured_output_schema.model_json_schema()\n",
    "\n",
    "        # For async calls\n",
    "        from ollama import AsyncClient\n",
    "\n",
    "        self.client = AsyncClient() if use_async else None\n",
    "\n",
    "        # Ensure model is pulled\n",
    "        self._ensure_model_available()\n",
    "\n",
    "        # Generate MCP tools in Ollama format\n",
    "        self.ollama_tools = self._convert_mcp_tools_to_ollama_format() if self.mcp_tools_enabled else []\n",
    "\n",
    "    @staticmethod\n",
    "    def get_available_models():\n",
    "        \"\"\"\n",
    "        Get a list of available Ollama models\n",
    "\n",
    "        Returns:\n",
    "            List[str]: List of model names\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import ollama\n",
    "\n",
    "            models = ollama.list()\n",
    "\n",
    "            # Extract model names from the list\n",
    "            model_names = [model.model for model in models[\"models\"]]\n",
    "            return model_names\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to get Ollama model list: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _ensure_model_available(self):\n",
    "        import ollama\n",
    "\n",
    "        try:\n",
    "            ollama.chat(\n",
    "                model=self.model_name, messages=[{\"role\": \"system\", \"content\": \"test\"}]\n",
    "            )\n",
    "        except ollama.ResponseError as e:\n",
    "            if e.status_code == 404:\n",
    "                self.logger.info(\n",
    "                    f\"Model {self.model_name} not found locally. Pulling...\"\n",
    "                )\n",
    "                ollama.pull(self.model_name)\n",
    "                self.logger.info(f\"Successfully pulled model {self.model_name}\")\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "    def _convert_mcp_tools_to_ollama_format(self) -> List[Dict]:\n",
    "        \"\"\"Convert MCP tools to Ollama tools format.\"\"\"\n",
    "        if not self.mcp_client:\n",
    "            return []\n",
    "            \n",
    "        ollama_tools = []\n",
    "        for tool in self.mcp_client.available_tools:\n",
    "            ollama_tool = {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": tool[\"name\"],\n",
    "                    \"description\": tool[\"description\"],\n",
    "                    \"parameters\": tool[\"input_schema\"]\n",
    "                }\n",
    "            }\n",
    "            ollama_tools.append(ollama_tool)\n",
    "        \n",
    "        return ollama_tools\n",
    "\n",
    "    def _process_ollama_tool_calls(self, ollama_tool_calls: List[Dict]) -> str:\n",
    "        \"\"\"Process Ollama tool calls and execute MCP tools.\"\"\"\n",
    "        if not ollama_tool_calls:\n",
    "            return \"\"\n",
    "            \n",
    "        results = []\n",
    "        \n",
    "        for i, tool_call in enumerate(ollama_tool_calls, 1):\n",
    "            if \"function\" in tool_call:\n",
    "                function_info = tool_call[\"function\"]\n",
    "                tool_name = function_info.get(\"name\")\n",
    "                arguments = function_info.get(\"arguments\", {})\n",
    "                \n",
    "                if not tool_name:\n",
    "                    results.append(f\"Tool call {i}: Missing function name\")\n",
    "                    continue\n",
    "                    \n",
    "                try:\n",
    "                    result = self.mcp_client.execute_tool(tool_name, **arguments)\n",
    "                    formatted_result = self.mcp_client.format_output(result)\n",
    "                    results.append(f\"Tool call {i} ({tool_name}):\\n{formatted_result}\")\n",
    "                except Exception as e:\n",
    "                    results.append(f\"Tool call {i} ({tool_name}) failed: {str(e)}\")\n",
    "        \n",
    "        return \"\\n\\n\".join(results)\n",
    "\n",
    "    def _prepare_options(self):\n",
    "        \"\"\"Common chat options for both sync and async calls.\"\"\"\n",
    "        opts = {\n",
    "            \"temperature\": self.temperature,\n",
    "            \"num_predict\": self.max_tokens,\n",
    "            \"num_ctx\": self.num_ctx,\n",
    "        }\n",
    "        chat_kwargs = {\"options\": opts}\n",
    "        if self.format_structured_output:\n",
    "            chat_kwargs[\"format\"] = self.format_structured_output\n",
    "        return chat_kwargs\n",
    "\n",
    "    def chat(\n",
    "        self,\n",
    "        messages: Union[List[Dict[str, Any]], Dict[str, Any]],\n",
    "        **kwargs,\n",
    "    ) -> Tuple[List[str], Usage, List[str]]:\n",
    "        \"\"\"\n",
    "        Handle synchronous chat completions. If you pass a list of message dicts,\n",
    "        we do one call for that entire conversation. If you pass a single dict,\n",
    "        we wrap it in a list so there's no error.\n",
    "        \"\"\"\n",
    "        if self.use_async:\n",
    "            return self.achat(messages, **kwargs)\n",
    "        else:\n",
    "            return self.schat(messages, **kwargs)\n",
    "\n",
    "    def schat(\n",
    "        self,\n",
    "        messages: Union[List[Dict[str, Any]], Dict[str, Any]],\n",
    "        **kwargs,\n",
    "    ) -> Tuple[List[str], Usage, List[str]]:\n",
    "        \"\"\"\n",
    "        Handle synchronous chat completions with optional MCP tool calling.\n",
    "        \"\"\"\n",
    "        import ollama\n",
    "\n",
    "        # If the user provided a single dictionary, wrap it\n",
    "        if isinstance(messages, dict):\n",
    "            messages = [messages]\n",
    "\n",
    "        # Create a copy of messages to avoid modifying the original\n",
    "        working_messages = messages.copy()\n",
    "        \n",
    "        chat_kwargs = self._prepare_options()\n",
    "        \n",
    "        # Add tools to chat kwargs if MCP is enabled\n",
    "        if self.mcp_tools_enabled and self.ollama_tools:\n",
    "            chat_kwargs[\"tools\"] = self.ollama_tools\n",
    "        responses = []\n",
    "        usage_total = Usage()\n",
    "        done_reasons = []\n",
    "        tools = []\n",
    "\n",
    "        # Tool calling loop\n",
    "        iteration = 0\n",
    "        while iteration < self.max_tool_iterations:\n",
    "\n",
    "            try:\n",
    "                response = ollama.chat(\n",
    "                    model=self.model_name,\n",
    "                    messages=working_messages,\n",
    "                    **chat_kwargs,\n",
    "                    **kwargs,\n",
    "                )\n",
    "                \n",
    "                response_content = response[\"message\"][\"content\"]\n",
    "                \n",
    "                # Track usage\n",
    "                try:\n",
    "                    usage_total += Usage(\n",
    "                        prompt_tokens=response[\"prompt_eval_count\"],\n",
    "                        completion_tokens=response[\"eval_count\"],\n",
    "                    )\n",
    "                except Exception:\n",
    "                    usage_total += Usage(prompt_tokens=0, completion_tokens=0)\n",
    "                \n",
    "                try:\n",
    "                    done_reasons.append(response[\"done_reason\"])\n",
    "                except Exception:\n",
    "                    done_reasons.append(\"stop\")\n",
    "\n",
    "                # Check if MCP tools are enabled and handle tool calls from Ollama response  \n",
    "                if self.mcp_tools_enabled and \"tool_calls\" in response[\"message\"]:\n",
    "                    ollama_tool_calls = response[\"message\"][\"tool_calls\"]\n",
    "                    \n",
    "                    if ollama_tool_calls:\n",
    "                        # Execute tools\n",
    "                        tool_results = self._process_ollama_tool_calls(ollama_tool_calls)\n",
    "                        \n",
    "                        # Add the model's response to conversation\n",
    "                        working_messages.append({\n",
    "                            \"role\": \"assistant\", \n",
    "                            \"content\": response_content,\n",
    "                            \"tool_calls\": ollama_tool_calls\n",
    "                        })\n",
    "                        \n",
    "                        # Add tool results as tool message\n",
    "                        for i, tool_call in enumerate(ollama_tool_calls):\n",
    "                            tool_name = tool_call.get(\"function\", {}).get(\"name\", f\"tool_{i}\")\n",
    "                            working_messages.append({\n",
    "                                \"role\": \"tool\",\n",
    "                                \"content\": tool_results,\n",
    "                                \"tool_call_id\": f\"call_{i}\"\n",
    "                            })\n",
    "                        \n",
    "                        iteration += 1\n",
    "                        continue  # Continue the loop for another iteration\n",
    "                \n",
    "                # No tool calls or tools disabled - this is the final response\n",
    "                responses.append(response_content)\n",
    "                \n",
    "                if \"tool_calls\" in response[\"message\"]:\n",
    "                    tools.append(response[\"message\"][\"tool_calls\"])\n",
    "                    \n",
    "                break\n",
    "\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error during Ollama API call: {e}\")\n",
    "                raise\n",
    "\n",
    "        # If we completed max iterations without a final response, use the last response\n",
    "        if not responses and iteration >= self.max_tool_iterations:\n",
    "            responses.append(\"Maximum tool iterations reached. Unable to provide final response.\")\n",
    "            done_reasons = [\"max_iterations\"]\n",
    "\n",
    "        if self.return_tools:\n",
    "            return responses, usage_total, done_reasons, tools\n",
    "        else:\n",
    "            if self.local:\n",
    "                return responses, usage_total, done_reasons\n",
    "            else:\n",
    "                return responses, usage_total\n",
    "\n",
    "    def embed(\n",
    "        self,\n",
    "        content,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Embed content using model (must support embeddings).\"\"\"\n",
    "        import ollama\n",
    "\n",
    "        response = ollama.embed(model=self.model_name, input=content, **kwargs)\n",
    "        return response[\"embeddings\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 設定 Ollama 主機地址指向你的 Docker 容器\n",
    "os.environ[\"OLLAMA_HOST\"] = \"http://192.168.96.2:11434\"\n",
    "\n",
    "remote_client = OpenAIClient(model_name=\"gpt-4o\", temperature=0.0)\n",
    "\n",
    "# Option 1: Ollama - 現在會使用環境變數中的主機地址\n",
    "local_client = OllamaClient(\n",
    "    model_name=\"llama3.2:3b\", \n",
    "    temperature=0.0, \n",
    "    structured_output_schema=StructuredLocalOutput\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up Communication Protocol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Optional, Union, Tuple\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pydantic import BaseModel, field_validator, Field\n",
    "from inspect import getsource\n",
    "\n",
    "from minions.utils.multimodal_retrievers import (\n",
    "    retrieve_chunks_from_chroma,\n",
    ")\n",
    "\n",
    "from minions.usage import Usage\n",
    "\n",
    "from minions.utils.chunking import (\n",
    "    chunk_by_section,\n",
    "    chunk_by_page,\n",
    "    chunk_by_paragraph,\n",
    "    extract_imports,\n",
    "    extract_function_header,\n",
    "    extract_function,\n",
    "    chunk_by_code,\n",
    "    chunk_by_function_and_class,\n",
    ")\n",
    "\n",
    "from minions.prompts.minions import (\n",
    "    WORKER_ICL_EXAMPLES,\n",
    "    WORKER_PROMPT_SHORT,\n",
    "    ADVICE_PROMPT,\n",
    "    DECOMPOSE_TASK_PROMPT_AGGREGATION_FUNC,\n",
    "    DECOMPOSE_TASK_PROMPT_AGG_FUNC_LATER_ROUND,\n",
    "    DECOMPOSE_RETRIEVAL_TASK_PROMPT_AGGREGATION_FUNC,\n",
    "    DECOMPOSE_RETRIEVAL_TASK_PROMPT_AGG_FUNC_LATER_ROUND,\n",
    "    REMOTE_SYNTHESIS_COT,\n",
    "    REMOTE_SYNTHESIS_JSON,\n",
    "    REMOTE_SYNTHESIS_FINAL,\n",
    "    BM25_INSTRUCTIONS,\n",
    "    EMBEDDING_INSTRUCTIONS,\n",
    ")\n",
    "\n",
    "from minions.utils.retrievers import (\n",
    "    bm25_retrieve_top_k_chunks,\n",
    "    embedding_retrieve_top_k_chunks,\n",
    ")\n",
    "\n",
    "def chunk_by_section(\n",
    "    doc: str, max_chunk_size: int = 3000, overlap: int = 20\n",
    ") -> List[str]:\n",
    "    sections = []\n",
    "    start = 0\n",
    "    while start < len(doc):\n",
    "        end = start + max_chunk_size\n",
    "        sections.append(doc[start:end])\n",
    "        start += max_chunk_size - overlap\n",
    "    return sections\n",
    "\n",
    "class JobManifest(BaseModel):\n",
    "    chunk: str  # the actual text for the chunk of the document\n",
    "    task: str  # the actual task instruction for the small model\n",
    "    advice: str  # optional, any additional advice on how to perform the task\n",
    "\n",
    "    chunk_id: Optional[int] = None  # you do NOT need to set this, it will be handled automatically\n",
    "    task_id: Optional[int] = None  # you do NOT need to set this, it will be handled automatically\n",
    "    job_id: Optional[int] = None  # you do NOT need to set this, it will be handled automatically\n",
    "\n",
    "class JobOutput(BaseModel):\n",
    "    explanation: str\n",
    "    citation: Optional[str]\n",
    "    answer: Optional[str]\n",
    "\n",
    "def prepare_jobs(\n",
    "    context: List[str],\n",
    "    prev_job_manifests: Optional[List[JobManifest]] = None,\n",
    "    prev_job_outputs: Optional[List[JobOutput]] = None,\n",
    ") -> List[JobManifest]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        context (List[str]): A list of documents. Assume each document is greater >100k tokens.\n",
    "            Each document can be further chunked using `chunk_pages`.\n",
    "            If context is empty, use the MCP functions to get information that you need to complete your task: i.e., ```context = mcp_tools.execute_tool(....)```\n",
    "        prev_job_manifests (Optional[List[JobManifest]]): A list of job manifests from the previous round.\n",
    "            None if we are on the first round.\n",
    "        prev_job_outputs (Optional[List[JobOutput]]): A list of job outputs from the previous round.\n",
    "            None if we are on the first round.\n",
    "    Returns:\n",
    "        List[JobManifest]: A list of job manifests for the current round.\n",
    "    \"\"\"\n",
    "    # 這裡是遠端模型生成的程式碼，會根據任務動態生成\n",
    "    pass\n",
    "\n",
    "class Job(BaseModel):\n",
    "    \"\"\"\n",
    "    An object for us to filter job manifests. not seen by the worker or used in the code block.\n",
    "    \"\"\"\n",
    "    manifest: JobManifest\n",
    "    output: JobOutput\n",
    "    sample: str  # this is the raw client sample\n",
    "    include: Optional[bool] = None\n",
    "\n",
    "def transform_outputs(\n",
    "    jobs: List[Job],\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        jobs (List[Job]): A list of jobs from the workers.\n",
    "    Returns:\n",
    "        str: A transformed view of all the job outputs (including answer, citation + explanation) that will be analyzed to make a final decision. Make sure to use **as much** information from the outputs as possible in final aggregated str (output.answer, output.sample, output.explanation, output.citation)\n",
    "\n",
    "        Note: Job has following attributes:\n",
    "        - manifest: JobManifest(chunk, task, advice, chunk_id, task_id, job_id)\n",
    "        - sample: entire response from the worker\n",
    "        - output: JobOutput(answer=\"\". explanation=\"\", citation=\"\", raw=\"\")\n",
    "    \"\"\"\n",
    "    # 這裡是遠端模型生成的程式碼，會根據任務動態生成\n",
    "    pass\n",
    "\n",
    "# these objects are passed to the exec_globals so the code block can use them without\n",
    "# having to import them itself\n",
    "USEFUL_IMPORTS = {\n",
    "    \"List\": List,\n",
    "    \"Optional\": Optional,\n",
    "    \"Dict\": Dict,\n",
    "    \"Any\": Any,\n",
    "    \"Union\": Union,\n",
    "    \"Tuple\": Tuple,\n",
    "    \"BaseModel\": BaseModel,\n",
    "    \"field_validator\": field_validator,\n",
    "}\n",
    "\n",
    "class Minions:\n",
    "    def __init__(\n",
    "        self,\n",
    "        local_client=None,\n",
    "        remote_client=None,\n",
    "        max_rounds=5,\n",
    "        callback=None,\n",
    "        log_dir=\"minions_logs\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Initialize the Minion with local and remote LLM clients.\n",
    "\n",
    "        Args:\n",
    "            local_client: Client for the local model (e.g. OllamaClient)\n",
    "            remote_client: Client for the remote model (e.g. OpenAIClient)\n",
    "            max_rounds: Maximum number of conversation rounds\n",
    "            callback: Optional callback function to receive message updates\n",
    "            log_dir: Directory for logging conversation history\n",
    "        \"\"\"\n",
    "        self.local_client = local_client\n",
    "        self.remote_client = remote_client\n",
    "        self.max_rounds = max_rounds\n",
    "        self.max_jobs_per_round = 2048\n",
    "        self.callback = callback\n",
    "        self.log_dir = log_dir\n",
    "        self.num_samples = 1 or kwargs.get(\"num_samples\", None)\n",
    "        self.worker_batch_size = 1 or kwargs.get(\"worker_batch_size\", None)\n",
    "        self.max_code_attempts = kwargs.get(\"max_code_attempts\", 10)\n",
    "        self.worker_prompt_template = WORKER_PROMPT_SHORT or kwargs.get(\n",
    "            \"worker_prompt_template\", None\n",
    "        )\n",
    "        self.worker_icl_examples = WORKER_ICL_EXAMPLES or kwargs.get(\n",
    "            \"worker_icl_examples\", None\n",
    "        )\n",
    "        self.worker_icl_messages = []\n",
    "        self.advice_prompt = ADVICE_PROMPT or kwargs.get(\"advice_prompt\", None)\n",
    "\n",
    "        self.decompose_task_prompt = (\n",
    "            kwargs.get(\"decompose_task_prompt\", None)\n",
    "            or DECOMPOSE_TASK_PROMPT_AGGREGATION_FUNC\n",
    "        )\n",
    "        self.decompose_task_prompt_abbreviated = (\n",
    "            kwargs.get(\"decompose_task_prompt_abbreviated\", None)\n",
    "            or DECOMPOSE_TASK_PROMPT_AGG_FUNC_LATER_ROUND\n",
    "        )\n",
    "        self.decompose_retrieval_task_prompt = (\n",
    "            kwargs.get(\"decompose_retrieval_task_prompt\", None)\n",
    "            or DECOMPOSE_RETRIEVAL_TASK_PROMPT_AGGREGATION_FUNC\n",
    "        )\n",
    "        self.decompose_retrieval_task_prompt_abbreviated = (\n",
    "            kwargs.get(\"decompose_retrieval_task_prompt_abbreviated\", None)\n",
    "            or DECOMPOSE_RETRIEVAL_TASK_PROMPT_AGG_FUNC_LATER_ROUND\n",
    "        )\n",
    "        self.synthesis_cot_prompt = REMOTE_SYNTHESIS_COT or kwargs.get(\n",
    "            \"synthesis_cot_prompt\", None\n",
    "        )\n",
    "        self.synthesis_json_prompt = REMOTE_SYNTHESIS_JSON or kwargs.get(\n",
    "            \"synthesis_json_prompt\", None\n",
    "        )\n",
    "        self.synthesis_final_prompt = REMOTE_SYNTHESIS_FINAL or kwargs.get(\n",
    "            \"synthesis_final_prompt\", None\n",
    "        )\n",
    "        self.chunking_fns = {\n",
    "            \"chunk_by_section\": chunk_by_section,\n",
    "            \"chunk_by_page\": chunk_by_page,\n",
    "            \"chunk_by_paragraph\": chunk_by_paragraph,\n",
    "            \"extract_imports\": extract_imports,\n",
    "            \"extract_function_header\": extract_function_header,\n",
    "            \"extract_function\": extract_function,\n",
    "            \"chunk_by_code\": chunk_by_code,\n",
    "            \"chunk_by_function_and_class\": chunk_by_function_and_class,\n",
    "        }\n",
    "        self.chunking_fn = chunk_by_section\n",
    "        # Create log directory if it doesn't exist\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    def _execute_code(\n",
    "        self,\n",
    "        code: str,\n",
    "        starting_globals: Dict[str, Any] = {},\n",
    "        fn_name: str = \"prepare_jobs\",\n",
    "        **kwargs,\n",
    "    ) -> Tuple[Any, str]:\n",
    "        exec_globals = {\n",
    "            **starting_globals\n",
    "        }  # dictionary to store variables in the code block\n",
    "        exec(code, exec_globals)  # first execution, with example usage\n",
    "        if fn_name not in exec_globals:\n",
    "            raise ValueError(f\"Function {fn_name} not found in the code block.\")\n",
    "        output = exec_globals[fn_name](\n",
    "            **kwargs\n",
    "        )  # by default, grab the prepare_jobs function, execute it with the kwargs, i.e., context\n",
    "\n",
    "        # call exec_globsl (filter_fnf)\n",
    "        return output, code\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        task: str,\n",
    "        doc_metadata: str,\n",
    "        context: List[str],\n",
    "        max_rounds=None,\n",
    "        max_jobs_per_round=None,\n",
    "        num_tasks_per_round=3,\n",
    "        num_samples_per_task=1,\n",
    "        mcp_tools_info=None,\n",
    "        use_retrieval=None,\n",
    "        log_path=None,\n",
    "        logging_id=None,\n",
    "        retrieval_model=None,\n",
    "        chunk_fn=\"chunk_by_section\",\n",
    "    ):\n",
    "        \"\"\"Run the minions protocol to answer a task using local and remote models.\n",
    "\n",
    "        Args:\n",
    "            task: The task/question to answer\n",
    "            doc_metadata: Type of document being analyzed\n",
    "            context: List of context strings\n",
    "            max_rounds: Override default max_rounds if provided\n",
    "            max_jobs_per_round: Override default max_jobs_per_round if provided\n",
    "            retrieval: Retrieval strategy to use. Options:\n",
    "                - None: Don't use retrieval\n",
    "                - \"bm25\": Use BM25 keyword-based retrieval\n",
    "                - \"embedding\": Use embedding-based retrieval\n",
    "            log_path: Optional path to save conversation logs\n",
    "\n",
    "        Returns:\n",
    "            Dict containing final_answer and conversation histories\n",
    "        \"\"\"\n",
    "\n",
    "        self.chunking_fn = self.chunking_fns[chunk_fn]\n",
    "\n",
    "        # Initialize timing metrics\n",
    "        start_time = time.time()\n",
    "        timing = {\n",
    "            \"local_call_time\": 0.0,\n",
    "            \"remote_call_time\": 0.0,\n",
    "            \"total_time\": 0.0,\n",
    "        }\n",
    "\n",
    "        print(\"\\n========== MINIONS TASK STARTED ==========\")\n",
    "        print(f\"Task: {task}\")\n",
    "        print(f\"Max rounds: {max_rounds or self.max_rounds}\")\n",
    "        print(f\"Retrieval: {use_retrieval}\")\n",
    "\n",
    "        self.max_rounds = max_rounds or self.max_rounds\n",
    "        self.max_jobs_per_round = max_jobs_per_round or self.max_jobs_per_round\n",
    "\n",
    "        # Initialize the log structure\n",
    "        conversation_log = {\n",
    "            \"task\": task,\n",
    "            \"doc_metadata\": doc_metadata,\n",
    "            \"conversation\": [],\n",
    "            \"generated_final_answer\": \"\",\n",
    "            \"usage\": {\n",
    "                \"remote\": {},\n",
    "                \"local\": {},\n",
    "            },\n",
    "            \"timing\": timing,\n",
    "        }\n",
    "\n",
    "        # Initialize usage tracking\n",
    "        remote_usage = Usage()\n",
    "        local_usage = Usage()\n",
    "\n",
    "        retriever = None\n",
    "        embedding_model_instance = None\n",
    "\n",
    "        if use_retrieval:\n",
    "            if use_retrieval == \"bm25\":\n",
    "                retriever = bm25_retrieve_top_k_chunks\n",
    "            elif use_retrieval == \"embedding\":\n",
    "                # Import SentenceTransformerEmbeddings here to avoid circular imports\n",
    "                from minions.utils.retrievers import SentenceTransformerEmbeddings, BaseEmbeddingModel\n",
    "                \n",
    "                # Handle retrieval_model parameter - can be model name string or model instance\n",
    "                if retrieval_model:\n",
    "                    if isinstance(retrieval_model, str):\n",
    "                        # If it's a string, treat it as a model name\n",
    "                        embedding_model_instance = SentenceTransformerEmbeddings(retrieval_model)\n",
    "                    elif isinstance(retrieval_model, BaseEmbeddingModel):\n",
    "                        # If it's already a model instance, use it directly\n",
    "                        embedding_model_instance = retrieval_model\n",
    "                    else:\n",
    "                        print(f\"Warning: retrieval_model should be a string or BaseEmbeddingModel instance, got {type(retrieval_model)}. Using default model.\")\n",
    "                        embedding_model_instance = SentenceTransformerEmbeddings()\n",
    "                else:\n",
    "                    embedding_model_instance = SentenceTransformerEmbeddings()\n",
    "                \n",
    "                # Store the embedding_model_instance to use later in starting_globals\n",
    "                retriever = embedding_retrieve_top_k_chunks\n",
    "            elif use_retrieval == \"multimodal-embedding\":\n",
    "                retriever = retrieve_chunks_from_chroma\n",
    "\n",
    "        # 1. [REMOTE] ADVICE --- Read the query with big model and provide advice\n",
    "        # ---------- START ----------\n",
    "        supervisor_messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": self.advice_prompt.format(query=task, metadata=doc_metadata),\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        # Add initial supervisor prompt to conversation log\n",
    "        conversation_log[\"conversation\"].append(\n",
    "            {\n",
    "                \"user\": \"supervisor\",\n",
    "                \"prompt\": self.advice_prompt.format(query=task, metadata=doc_metadata),\n",
    "                \"output\": None,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if self.callback:\n",
    "            self.callback(\"supervisor\", None, is_final=False)\n",
    "\n",
    "        remote_start_time = time.time()\n",
    "        advice_response, usage = self.remote_client.chat(\n",
    "            supervisor_messages,\n",
    "        )\n",
    "        current_time = time.time()\n",
    "        timing[\"remote_call_time\"] += current_time - remote_start_time\n",
    "\n",
    "        remote_usage += usage\n",
    "\n",
    "        supervisor_messages.append(\n",
    "            {\"role\": \"assistant\", \"content\": advice_response[0]},\n",
    "        )\n",
    "\n",
    "        # Update conversation log with response\n",
    "        conversation_log[\"conversation\"][-1][\"output\"] = advice_response[0]\n",
    "\n",
    "        if self.callback:\n",
    "            self.callback(\"supervisor\", supervisor_messages[-1], is_final=False)\n",
    "        # ---------- END ----------\n",
    "\n",
    "        last_jobs: Optional[List[Job]] = None\n",
    "        feedback: Optional[str] = None\n",
    "        scratchpad: str = \"\"\n",
    "        meta: List[Dict[str, any]] = []\n",
    "        final_answer: Optional[str] = None\n",
    "\n",
    "        for round_idx in range(self.max_rounds):\n",
    "            print(f\"Round {round_idx + 1}/{self.max_rounds}\")\n",
    "            try:\n",
    "                total_chars = int(\n",
    "                    doc_metadata.split(\"Total extracted text length: \")[1].split(\n",
    "                        \" characters\"\n",
    "                    )[0]\n",
    "                )\n",
    "            except:\n",
    "                # compute characters in context\n",
    "                total_chars = sum(len(doc) for doc in context)\n",
    "\n",
    "            retrieval_source = \"\"\n",
    "            retrieval_instructions = \"\"\n",
    "\n",
    "            if use_retrieval:\n",
    "                retrieval_source = getsource(retriever).split(\"    weights = \")[0]\n",
    "\n",
    "                retrieval_instructions = (\n",
    "                    BM25_INSTRUCTIONS\n",
    "                    if use_retrieval == \"bm25\"\n",
    "                    else EMBEDDING_INSTRUCTIONS if use_retrieval == \"embedding\" else \"\"\n",
    "                )\n",
    "\n",
    "            print(getsource(self.chunking_fn))\n",
    "            decompose_message_kwargs = dict(\n",
    "                num_samples=self.num_samples,\n",
    "                ADVANCED_STEPS_INSTRUCTIONS=\"\",\n",
    "                manifest_source=getsource(JobManifest),\n",
    "                output_source=getsource(JobOutput),\n",
    "                signature_source=getsource(prepare_jobs),\n",
    "                transform_signature_source=getsource(transform_outputs),\n",
    "                chunking_source=\"\\n\\n\".join(\n",
    "                    [getsource(self.chunking_fn).split(\"    sections = \")[0]]\n",
    "                ),\n",
    "                retrieval_source=retrieval_source,\n",
    "                retrieval_instructions=retrieval_instructions,\n",
    "                num_tasks_per_round=num_tasks_per_round,\n",
    "                num_samples_per_task=num_samples_per_task,\n",
    "                total_chars=total_chars,\n",
    "            )\n",
    "\n",
    "            decompose_prompt = (\n",
    "                self.decompose_task_prompt\n",
    "                if not use_retrieval\n",
    "                else self.decompose_retrieval_task_prompt\n",
    "            )\n",
    "            # create the decompose prompt -- if in later rounds, use a shorter version\n",
    "            decompose_message = {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": decompose_prompt.format(\n",
    "                    step_number=1,\n",
    "                    mcp_tools_info=mcp_tools_info,\n",
    "                    **decompose_message_kwargs,\n",
    "                ),\n",
    "            }\n",
    "\n",
    "            if round_idx == 0:\n",
    "                supervisor_messages.append(decompose_message)\n",
    "            else:\n",
    "                decompose_prompt_abbrev = (\n",
    "                    self.decompose_task_prompt_abbreviated\n",
    "                    if not use_retrieval\n",
    "                    else self.decompose_retrieval_task_prompt_abbreviated\n",
    "                )\n",
    "                if feedback is not None:\n",
    "                    decompose_message = {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": decompose_prompt_abbrev.format(\n",
    "                            step_number=round_idx + 1,\n",
    "                            feedback=feedback,\n",
    "                            scratchpad=scratchpad,\n",
    "                            mcp_tools_info=mcp_tools_info,\n",
    "                            **decompose_message_kwargs,\n",
    "                        ),\n",
    "                    }\n",
    "                supervisor_messages = supervisor_messages[:2] + [decompose_message]\n",
    "\n",
    "            # Add decompose prompt to conversation log\n",
    "            conversation_log[\"conversation\"].append(\n",
    "                {\n",
    "                    \"user\": \"supervisor\",\n",
    "                    \"prompt\": decompose_message[\"content\"],\n",
    "                    \"output\": None,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # 2. [REMOTE] PREPARE TASKS --- Prompt the supervisor to write code\n",
    "            # ---------- START ----------\n",
    "            for attempt_idx in range(self.max_code_attempts):\n",
    "                print(f\"Attempt {attempt_idx + 1}/{self.max_code_attempts}\")\n",
    "\n",
    "                if self.callback:\n",
    "                    self.callback(\"supervisor\", None, is_final=False)\n",
    "\n",
    "                remote_start_time = time.time()\n",
    "                task_response, usage = self.remote_client.chat(\n",
    "                    messages=supervisor_messages,\n",
    "                )\n",
    "                current_time = time.time()\n",
    "                timing[\"remote_call_time\"] += current_time - remote_start_time\n",
    "\n",
    "                remote_usage += usage\n",
    "\n",
    "                task_response = task_response[0]\n",
    "                print(task_response)\n",
    "                supervisor_messages.append(\n",
    "                    {\"role\": \"assistant\", \"content\": task_response},\n",
    "                )\n",
    "\n",
    "                # Update conversation log with response\n",
    "                conversation_log[\"conversation\"][-1][\"output\"] = task_response\n",
    "\n",
    "                if self.callback:\n",
    "                    self.callback(\"supervisor\", supervisor_messages[-1], is_final=False)\n",
    "\n",
    "                code_block_match = re.search(\n",
    "                    r\"```(?:python)?\\s*(.*?)```\",\n",
    "                    task_response,\n",
    "                    re.DOTALL,\n",
    "                )\n",
    "\n",
    "                if code_block_match:\n",
    "                    code_block = code_block_match.group(1).strip()\n",
    "                else:\n",
    "                    print(f\"No code block found in the supervisor response.\")\n",
    "                    supervisor_messages.append(\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": f\"Please try again. No code block found in the supervisor response.\",\n",
    "                        }\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "                # prepare the inputs for the code execution\n",
    "                starting_globals = {\n",
    "                    **USEFUL_IMPORTS,\n",
    "                    \"chunk_by_section\": chunk_by_section,\n",
    "                    f\"{chunk_fn}\": self.chunking_fn,\n",
    "                    \"JobManifest\": JobManifest,\n",
    "                    \"JobOutput\": JobOutput,\n",
    "                    \"Job\": Job,\n",
    "                }\n",
    "\n",
    "                if use_retrieval:\n",
    "                    if use_retrieval == \"embedding\" and embedding_model_instance:\n",
    "                        # Create a partial function that includes the embedding model\n",
    "                        def embedding_retriever_with_model(*args, **kwargs):\n",
    "                            return embedding_retrieve_top_k_chunks(*args, embedding_model=embedding_model_instance, **kwargs)\n",
    "                        starting_globals[f\"{use_retrieval}_retrieve_top_k_chunks\"] = embedding_retriever_with_model\n",
    "                    else:\n",
    "                        starting_globals[f\"{use_retrieval}_retrieve_top_k_chunks\"] = retriever\n",
    "\n",
    "                fn_kwargs = {\n",
    "                    \"context\": context,\n",
    "                    \"prev_job_manifests\": (\n",
    "                        [job.manifest for job in last_jobs]\n",
    "                        if last_jobs is not None\n",
    "                        else None\n",
    "                    ),\n",
    "                    \"prev_job_outputs\": (\n",
    "                        [job.output for job in last_jobs]\n",
    "                        if last_jobs is not None\n",
    "                        else None\n",
    "                    ),\n",
    "                }\n",
    "                try:\n",
    "                    job_manifests, compiled_code_block = self._execute_code(\n",
    "                        code_block,\n",
    "                        starting_globals=starting_globals,\n",
    "                        fn_name=\"prepare_jobs\",  # the global variable to extract from the code block\n",
    "                        **fn_kwargs,\n",
    "                    )\n",
    "\n",
    "                    # We need to coerce the type below to ensure that the type is\n",
    "                    # not a different `JobManifest` object the model defined in it's\n",
    "                    # own code. We also need to set the chunk_id and task_id.\n",
    "                    chunk_ids, task_ids = {}, {}\n",
    "                    job_manifests = [\n",
    "                        JobManifest(\n",
    "                            chunk=job_manifest.chunk,\n",
    "                            task=job_manifest.task,\n",
    "                            advice=job_manifest.advice,\n",
    "                            chunk_id=chunk_ids.setdefault(\n",
    "                                job_manifest.chunk, len(chunk_ids)\n",
    "                            ),\n",
    "                            task_id=task_ids.setdefault(\n",
    "                                job_manifest.task, len(task_ids)\n",
    "                            ),\n",
    "                            job_id=job_id,\n",
    "                        )\n",
    "                        for job_id, job_manifest in enumerate(job_manifests)\n",
    "                    ]\n",
    "\n",
    "                    if len(job_manifests) > self.max_jobs_per_round:\n",
    "                        print(\n",
    "                            f\"Exceeded max jobs per round: {len(job_manifests)} > {self.max_jobs_per_round}. Trying again.\"\n",
    "                        )\n",
    "                        supervisor_messages.append(\n",
    "                            {\n",
    "                                \"role\": \"user\",\n",
    "                                \"content\": f\"Your code is output {len(job_manifests)} jobs which exceeds the max jobs per round ({self.max_jobs_per_round}). Please try again.\",\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "                        # Log this error to conversation log\n",
    "                        conversation_log[\"conversation\"].append(\n",
    "                            {\n",
    "                                \"user\": \"supervisor\",\n",
    "                                \"prompt\": f\"Your code is output {len(job_manifests)} jobs which exceeds the max jobs per round ({self.max_jobs_per_round}). Please try again.\",\n",
    "                                \"output\": None,\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "                        continue\n",
    "                    print(\n",
    "                        f\"Created {len(job_manifests)} job manifests ({len(chunk_ids)} chunks, apriori requested {self.num_samples} samples per chunk, {len(task_ids)} tasks)\"\n",
    "                    )\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(\n",
    "                        f\"Error executing code (attempt {attempt_idx} of {self.max_code_attempts} max attempts): {type(e).__name__}: {e}\"\n",
    "                    )\n",
    "\n",
    "                    error_message = f\"Please try again. I got this error when executing the code: \\n\\n```{type(e).__name__}: {e}```\"\n",
    "                    supervisor_messages.append(\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": error_message,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                    # Log this error to conversation log\n",
    "                    conversation_log[\"conversation\"].append(\n",
    "                        {\n",
    "                            \"user\": \"supervisor\",\n",
    "                            \"prompt\": error_message,\n",
    "                            \"output\": None,\n",
    "                        }\n",
    "                    )\n",
    "            else:\n",
    "                # if we have exhausted all attempts, break\n",
    "                print(\n",
    "                    f\"Exhausted all attempts to execute code. Breaking out of round loop.\"\n",
    "                )\n",
    "                break\n",
    "            # --------- END ---------\n",
    "\n",
    "            # 3. [REMOTE] LOCAL WORKERS EXECUTE TASKS\n",
    "            # ---------- START ----------\n",
    "            worker_chats = []\n",
    "            # output is a list of task_dicts\n",
    "            # print total number of job_manfiests\n",
    "            print(f\"Total number of job_manifests: {len(job_manifests)}\")\n",
    "            for job_manifest in job_manifests:\n",
    "                # Each worker is going to see a unique task+chunk combo\n",
    "                # removed the external list\n",
    "                worker_messages = {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": self.worker_prompt_template.format(\n",
    "                        context=job_manifest.chunk,\n",
    "                        task=job_manifest.task,\n",
    "                        advice=job_manifest.advice,\n",
    "                    ),\n",
    "                }\n",
    "                worker_chats.append(worker_messages)\n",
    "\n",
    "            if self.callback:\n",
    "                self.callback(\"worker\", None, is_final=False)\n",
    "\n",
    "            print(f\"Sending {len(worker_chats)} worker chats to the worker client\")\n",
    "\n",
    "            # Add worker tasks to conversation log\n",
    "            conversation_log[\"conversation\"].append(\n",
    "                {\n",
    "                    \"user\": \"worker\",\n",
    "                    \"prompt\": f\"Sending {len(worker_chats)} worker chats\",\n",
    "                    \"output\": None,\n",
    "                    \"job_manifests\": [job.model_dump() for job in job_manifests],\n",
    "                }\n",
    "            )\n",
    "\n",
    "            local_start_time = time.time()\n",
    "            worker_response, usage, done_reasons = self.local_client.chat(\n",
    "                worker_chats,\n",
    "            )\n",
    "            current_time = time.time()\n",
    "            timing[\"local_call_time\"] += current_time - local_start_time\n",
    "\n",
    "            local_usage += usage\n",
    "\n",
    "            def extract_job_output(response: str) -> JobOutput:\n",
    "                output = JobOutput.model_validate_json(response)\n",
    "                return output\n",
    "\n",
    "            jobs: List[Job] = []\n",
    "            for worker_messages, sample, job_manifest, done_reason in zip(\n",
    "                worker_chats, worker_response, job_manifests, done_reasons\n",
    "            ):\n",
    "                if done_reason == \"length\":\n",
    "                    job_output = JobOutput(\n",
    "                        answer=None,\n",
    "                        explanation=\"The model returned a truncated response. Please try again.\",\n",
    "                        citation=None,\n",
    "                    )\n",
    "                    continue\n",
    "                elif done_reason == \"stop\":\n",
    "                    job_output = extract_job_output(response=sample)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown done reason: {done_reason}\")\n",
    "                jobs.append(\n",
    "                    Job(\n",
    "                        manifest=job_manifest,\n",
    "                        sample=sample,\n",
    "                        output=job_output,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            fn_kwargs = {\n",
    "                \"jobs\": jobs,\n",
    "            }\n",
    "            if self.callback:\n",
    "                self.callback(\"worker\", jobs, is_final=False)\n",
    "\n",
    "            # Update conversation log with worker responses\n",
    "            conversation_log[\"conversation\"][-1][\"output\"] = [\n",
    "                job.sample for job in jobs\n",
    "            ]\n",
    "            conversation_log[\"conversation\"][-1][\"job_outputs\"] = [\n",
    "                {\n",
    "                    \"job_id\": job.manifest.job_id,\n",
    "                    \"chunk_id\": job.manifest.chunk_id,\n",
    "                    \"task_id\": job.manifest.task_id,\n",
    "                    \"output\": job.output.model_dump(),\n",
    "                }\n",
    "                for job in jobs\n",
    "            ]\n",
    "\n",
    "            try:\n",
    "                # Model generated Filter + Aggregation code\n",
    "                for job in jobs:\n",
    "                    print(job.output.answer)\n",
    "\n",
    "                aggregated_str, code_block = self._execute_code(\n",
    "                    code_block,\n",
    "                    starting_globals=starting_globals,\n",
    "                    fn_name=\"transform_outputs\",  # the global variable to extract from the code block\n",
    "                    **fn_kwargs,\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                # Log exception\n",
    "                print(f\"Error executing transformation code: {type(e).__name__}: {e}\")\n",
    "                conversation_log[\"conversation\"].append(\n",
    "                    {\n",
    "                        \"user\": \"supervisor\",\n",
    "                        \"prompt\": f\"Error executing transformation code: {type(e).__name__}: {e}\",\n",
    "                        \"output\": None,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                # 4. [EDGE] FILTER\n",
    "                # ---------- START ----------\n",
    "                def filter_fn(job: Job) -> bool:\n",
    "                    answer = job.output.answer\n",
    "                    if answer is None or str(answer).lower().strip() == \"none\":\n",
    "                        return False\n",
    "                    return True\n",
    "\n",
    "                for job in jobs:\n",
    "                    job.include = filter_fn(job)\n",
    "\n",
    "                print(\n",
    "                    f\"After filtering, {sum(job.include for job in jobs)}/{len(jobs)} jobs were included\"\n",
    "                )\n",
    "                # ---------- END ----------\n",
    "\n",
    "                # 5. [REMOTE] AGGREGATE AND FILTER --- Synthesize the results from the worker models\n",
    "                # ---------- START ----------\n",
    "                tasks = {}\n",
    "                for job in jobs:\n",
    "                    # 1. Create a container for each task_id if it doesn't exist yet.\n",
    "                    if job.manifest.task_id not in tasks:\n",
    "                        tasks[job.manifest.task_id] = {\n",
    "                            \"task_id\": job.manifest.task_id,\n",
    "                            \"task\": job.manifest.task,  # <-- Store the actual task string here\n",
    "                            \"chunks\": {},  # <-- We'll group by chunk_id next\n",
    "                        }\n",
    "\n",
    "                    # 2. For the given task_id, group by chunk_id\n",
    "                    c_id = job.manifest.chunk_id\n",
    "                    if c_id not in tasks[job.manifest.task_id][\"chunks\"]:\n",
    "                        tasks[job.manifest.task_id][\"chunks\"][c_id] = []\n",
    "\n",
    "                    tasks[job.manifest.task_id][\"chunks\"][c_id].append(job)\n",
    "\n",
    "                # Step 2: Build the string to pass to the big model,\n",
    "                # grouping by task first and then by chunk.\n",
    "                aggregated_str = \"\"\n",
    "                for task_id, task_info in tasks.items():\n",
    "                    aggregated_str += (\n",
    "                        f\"## Task (task_id=`{task_id}`): {task_info['task']}\\n\\n\"\n",
    "                    )\n",
    "                    # task_info['task'] is the string you saved above.\n",
    "\n",
    "                    # Inside each task, go chunk by chunk.\n",
    "                    for chunk_id, chunk_jobs in task_info[\"chunks\"].items():\n",
    "                        # Filter out any jobs that failed or are flagged \"include=False\".\n",
    "                        filtered_jobs = [j for j in chunk_jobs if j.include]\n",
    "\n",
    "                        if filtered_jobs:\n",
    "                            aggregated_str += f\"### Chunk # {chunk_id}\\n\"\n",
    "                            for idx, job in enumerate(filtered_jobs, start=1):\n",
    "                                aggregated_str += f\"   -- Job {idx} (job_id=`{job.manifest.job_id}`):\\n\"\n",
    "                                aggregated_str += f\"   {job.sample}\\n\\n\"\n",
    "                        else:\n",
    "                            aggregated_str += f\"### Chunk # {chunk_id}\\n\"\n",
    "                            aggregated_str += (\n",
    "                                \"   No jobs returned successfully for this chunk.\\n\\n\"\n",
    "                            )\n",
    "                    # Separate tasks with a short delimiter\n",
    "                    aggregated_str += \"\\n-----------------------\\n\\n\"\n",
    "\n",
    "            if round_idx == self.max_rounds - 1:\n",
    "                # Final round - use the final prompt directly\n",
    "                final_prompt = self.synthesis_final_prompt.format(\n",
    "                    extractions=aggregated_str,\n",
    "                    question=task,\n",
    "                    scratchpad=(scratchpad if scratchpad else \"No previous progress.\"),\n",
    "                )\n",
    "                supervisor_messages.append(\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": final_prompt,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                # Add synthesis prompt to conversation log\n",
    "                conversation_log[\"conversation\"].append(\n",
    "                    {\n",
    "                        \"user\": \"supervisor\",\n",
    "                        \"prompt\": final_prompt,\n",
    "                        \"output\": None,\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                # First step: Think through the synthesis\n",
    "                cot_prompt = self.synthesis_cot_prompt.format(\n",
    "                    extractions=aggregated_str,\n",
    "                    question=task,\n",
    "                    scratchpad=(scratchpad if scratchpad else \"No previous progress.\"),\n",
    "                )\n",
    "                supervisor_messages.append(\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": cot_prompt,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                # Add COT prompt to conversation log\n",
    "                conversation_log[\"conversation\"].append(\n",
    "                    {\n",
    "                        \"user\": \"supervisor\",\n",
    "                        \"prompt\": cot_prompt,\n",
    "                        \"output\": None,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                remote_start_time = time.time()\n",
    "                step_by_step_response, usage = self.remote_client.chat(\n",
    "                    supervisor_messages,\n",
    "                )\n",
    "                current_time = time.time()\n",
    "                timing[\"remote_call_time\"] += current_time - remote_start_time\n",
    "\n",
    "                remote_usage += usage\n",
    "                if self.callback:\n",
    "                    self.callback(\"supervisor\", step_by_step_response[0], is_final=False)\n",
    "\n",
    "                supervisor_messages.append(\n",
    "                    {\"role\": \"assistant\", \"content\": step_by_step_response[0]}\n",
    "                )\n",
    "\n",
    "                # Update conversation log with COT response\n",
    "                conversation_log[\"conversation\"][-1][\"output\"] = step_by_step_response[\n",
    "                    0\n",
    "                ]\n",
    "\n",
    "                # Second step: Get structured output\n",
    "                json_prompt = self.synthesis_json_prompt\n",
    "                supervisor_messages.append(\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": json_prompt,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                # Add JSON prompt to conversation log\n",
    "                conversation_log[\"conversation\"].append(\n",
    "                    {\n",
    "                        \"user\": \"supervisor\",\n",
    "                        \"prompt\": json_prompt,\n",
    "                        \"output\": None,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # Get the structured output and validate JSON response\n",
    "            max_attempts = 5\n",
    "            for attempt_idx in range(max_attempts):\n",
    "                try:\n",
    "                    if self.callback:\n",
    "                        self.callback(\"supervisor\", None, is_final=False)\n",
    "                    # Request JSON response from remote client\n",
    "                    remote_start_time = time.time()\n",
    "                    synthesized_response, usage = self.remote_client.chat(\n",
    "                        supervisor_messages, response_format={\"type\": \"json_object\"}\n",
    "                    )\n",
    "                    current_time = time.time()\n",
    "                    timing[\"remote_call_time\"] += current_time - remote_start_time\n",
    "\n",
    "                    # Parse and validate JSON response\n",
    "                    response_text = synthesized_response[0]\n",
    "                    print(\n",
    "                        f\"Attempt {attempt_idx + 1}/{max_attempts} response: {response_text}\"\n",
    "                    )\n",
    "\n",
    "                    obj = json.loads(response_text)\n",
    "                    if not isinstance(obj, dict) or \"decision\" not in obj:\n",
    "                        raise ValueError(\"Response missing required 'decision' field\")\n",
    "\n",
    "                    # Valid JSON with decision field found\n",
    "                    break\n",
    "\n",
    "                except (json.JSONDecodeError, ValueError) as e:\n",
    "                    print(f\"Attempt {attempt_idx + 1}/{max_attempts} failed: {str(e)}\")\n",
    "                    if attempt_idx == max_attempts - 1:\n",
    "                        raise ValueError(\n",
    "                            f\"Failed to get valid JSON response after {max_attempts} attempts\"\n",
    "                        )\n",
    "\n",
    "            supervisor_messages.append(\n",
    "                {\"role\": \"assistant\", \"content\": synthesized_response[0]}\n",
    "            )\n",
    "\n",
    "            # Update conversation log with synthesis response\n",
    "            conversation_log[\"conversation\"][-1][\"output\"] = synthesized_response[0]\n",
    "\n",
    "            if self.callback:\n",
    "                self.callback(\"supervisor\", supervisor_messages[-1], is_final=True)\n",
    "            # ---------- END ----------\n",
    "\n",
    "            last_jobs = jobs\n",
    "\n",
    "            meta.append(\n",
    "                {\n",
    "                    \"local\": {\n",
    "                        \"jobs\": [\n",
    "                            {k: v for k, v in job.model_dump().items() if k != \"sample\"}\n",
    "                            for job in jobs\n",
    "                        ]\n",
    "                    },\n",
    "                    \"remote\": {\"messages\": supervisor_messages},\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if obj[\"decision\"] != \"request_additional_info\":\n",
    "                final_answer = obj.get(\"answer\", None)\n",
    "                conversation_log[\"generated_final_answer\"] = final_answer\n",
    "                \n",
    "                # Send final answer callback\n",
    "                if self.callback and final_answer:\n",
    "                    self.callback(\"supervisor\", {\"role\": \"assistant\", \"content\": final_answer}, is_final=True)\n",
    "                \n",
    "                break  # answer was found, so we are done!\n",
    "            else:\n",
    "                feedback = obj.get(\"explanation\", None)\n",
    "                scratchpad = obj.get(\"scratchpad\", None)\n",
    "\n",
    "        if final_answer == None:\n",
    "            print(\n",
    "                f\"Exhausted all rounds without finding a final answer. Returning the last synthesized response.\"\n",
    "            )\n",
    "            final_answer = \"No answer found.\"\n",
    "            conversation_log[\"generated_final_answer\"] = final_answer\n",
    "\n",
    "        # Calculate total time at the end\n",
    "        end_time = time.time()\n",
    "        timing[\"total_time\"] = end_time - start_time\n",
    "        timing[\"overhead_time\"] = timing[\"total_time\"] - (\n",
    "            timing[\"local_call_time\"] + timing[\"remote_call_time\"]\n",
    "        )\n",
    "\n",
    "        # Add usage statistics to the conversation log\n",
    "        conversation_log[\"usage\"][\"remote\"] = remote_usage.to_dict()\n",
    "        conversation_log[\"usage\"][\"local\"] = local_usage.to_dict()\n",
    "\n",
    "        # Save the conversation log to a file\n",
    "        if log_path:\n",
    "            os.makedirs(os.path.dirname(log_path), exist_ok=True)\n",
    "            with open(log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(conversation_log, f, indent=2, ensure_ascii=False)\n",
    "        else:\n",
    "            # Create a log filename based on timestamp and task or provided logging_id\n",
    "            if logging_id:\n",
    "                log_filename = f\"{logging_id}_minions.json\"\n",
    "            else:\n",
    "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                safe_task = re.sub(r\"[^a-zA-Z0-9]\", \"_\", task[:15])\n",
    "                log_filename = f\"{timestamp}_{safe_task}_minions.json\"\n",
    "\n",
    "            log_path = os.path.join(self.log_dir, log_filename)\n",
    "\n",
    "            # Create the directory if it doesn't exist\n",
    "            os.makedirs(os.path.dirname(log_path), exist_ok=True)\n",
    "\n",
    "            # Save the log file\n",
    "            with open(log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(conversation_log, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "            print(f\"\\n=== SAVED LOG TO {log_path} ===\")\n",
    "\n",
    "        print(\"\\n=== MINIONS TASK COMPLETED ===\")\n",
    "\n",
    "        result = {\n",
    "            \"final_answer\": final_answer,\n",
    "            \"meta\": meta,\n",
    "            \"log_file\": log_path,\n",
    "            \"conversation_log\": conversation_log,\n",
    "            \"timing\": timing,\n",
    "            \"local_usage\": local_usage.to_dict(),\n",
    "            \"remote_usage\": remote_usage.to_dict(),\n",
    "        }\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocol = Minions(local_client=local_client, remote_client=remote_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Minion/Minions Task\n",
    "\n",
    "- context (List[str]): context that the minions need to reason over\n",
    "- doc_metadata (str): every task is parameterized by doc_metadata which describes the \"type\" of information contained in the context\n",
    "- task (str): description of the query to be completed over the context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Specify Input Context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\\\n",
    "Patient Name: John A. Doe\n",
    "Medical Record Number: 001234567\n",
    "DOB: 1967-08-22\n",
    "Gender: Male\n",
    "Admission Date: 2025-01-15\n",
    "Discharge Date: 2025-01-20\n",
    "\n",
    "Chief Complaint:\n",
    "The patient presented with chest discomfort, shortness of breath, and fatigue. Symptoms began gradually 48 hours before admission and intensified over time.\n",
    "\n",
    "History of Present Illness:\n",
    "John A. Doe, a 57-year-old male with a history of type 2 diabetes mellitus and hypertension, was admitted following several episodes of atypical chest pain. He experienced intermittent chest tightness and dyspnea on exertion. The pain did not radiate, and there was no associated nausea.\n",
    "\n",
    "Past Medical History:\n",
    "- Type 2 Diabetes Mellitus (diagnosed 2010) – managed with metformin.\n",
    "- Hypertension (diagnosed 2012) – treated with lisinopril.\n",
    "- Hyperlipidemia – treated with atorvastatin.\n",
    "- Former smoker (quit 2015).\n",
    "- No known drug allergies.\n",
    "\n",
    "Medications on Admission:\n",
    "- Metformin 1000 mg twice daily.\n",
    "- Lisinopril 20 mg daily.\n",
    "- Atorvastatin 40 mg nightly.\n",
    "- Aspirin 81 mg daily.\n",
    "\n",
    "Physical Examination:\n",
    "General: The patient is alert and oriented, in moderate distress.\n",
    "Cardiovascular: Blood pressure 150/95 mmHg; heart rate 88 bpm; regular rhythm.\n",
    "Respiratory: Lungs are clear to auscultation bilaterally; respiratory rate 16 bpm.\n",
    "Neurological: No focal deficits observed.\n",
    "\n",
    "Laboratory and Diagnostic Findings (Admission):\n",
    "- Complete Blood Count: WBC 7.2 x10^9/L, Hemoglobin 13.5 g/dL.\n",
    "- Basic Metabolic Panel: Serum creatinine 1.0 mg/dL; electrolytes normal.\n",
    "- Fasting Blood Glucose: 180 mg/dL.\n",
    "- Hemoglobin A1C: 8.5%.\n",
    "- Lipid Profile: Total Cholesterol 220 mg/dL, LDL 140 mg/dL, HDL 35 mg/dL.\n",
    "- ECG: Non-specific ST-T changes.\n",
    "- Chest X-ray: No acute findings.\n",
    "\n",
    "Progress Notes:\n",
    "Day 2: Initiated subcutaneous insulin to complement oral hypoglycemics. Blood pressure remained elevated at 148/92 mmHg.\n",
    "Day 3: Follow-up labs showed a minor improvement in glycemic levels and stable kidney function.\n",
    "Day 5: Vital signs improved with a blood pressure of 140/85 mmHg and heart rate of 80 bpm. A repeat lab indicated that the fasting blood glucose had decreased significantly compared to admission.\n",
    "Day 6: Endocrinology recommended maintaining current treatment with a possible gradual reduction in insulin if improvements continue.\n",
    "Day 7: Patient stabilized and was discharged with clear instructions for follow-up with both primary care and endocrinology.\n",
    "\n",
    "Discharge Summary:\n",
    "John A. Doe was discharged on Day 7 after noticeable improvements in glycemic control and blood pressure. The patient is to continue current medications with a scheduled outpatient review.\n",
    "Medications at Discharge:\n",
    "- Metformin 1000 mg BID.\n",
    "- Lisinopril 20 mg daily.\n",
    "- Atorvastatin 40 mg nightly.\n",
    "- Adjusted insulin regimen based on recent glycemic trends.\n",
    "\n",
    "Additional Information:\n",
    "- Admission weight: 95 kg (BMI: 31.2).\n",
    "- Dietary consult completed; patient advised on a diabetic diet.\n",
    "- Patient educated on adherence and regular blood glucose monitoring.\n",
    "\n",
    "Summary:\n",
    "This electronic health record details John A. Doe's hospital admission, clinical findings, laboratory values, treatments, and progress over a 7-day period. Key values include a fasting blood glucose of 180 mg/dL on admission and 150 mg/dL on Day 5, indicating a notable improvement in glycemic control.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Specify Metadata and Task Description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_metadata = \"Patient Visit Notes\"\n",
    "\n",
    "# Define the task without explicitly providing the key values.\n",
    "task = (\n",
    "    \"Using the electronic health records provided, calculate the percentage decrease in the patient's fasting \"\n",
    "    \"blood glucose level from admission to Day 5. Extract the necessary values from the record, show your calculations, \"\n",
    "    \"and provide the final percentage decrease.\"\n",
    ")\n",
    "\n",
    "# For testing purposes, we suggest that the correct answer is:\n",
    "suggested_final_answer = (\n",
    "    \"16.67% decrease in fasting blood glucose from admission to Day 5.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the protocol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== MINIONS TASK STARTED ==========\n",
      "Task: Using the electronic health records provided, calculate the percentage decrease in the patient's fasting blood glucose level from admission to Day 5. Extract the necessary values from the record, show your calculations, and provide the final percentage decrease.\n",
      "Max rounds: 5\n",
      "Retrieval: None\n",
      "Round 1/5\n",
      "def chunk_by_section(\n",
      "    doc: str, max_chunk_size: int = 3000, overlap: int = 20\n",
      ") -> List[str]:\n",
      "    sections = []\n",
      "    start = 0\n",
      "    while start < len(doc):\n",
      "        end = start + max_chunk_size\n",
      "        sections.append(doc[start:end])\n",
      "        start += max_chunk_size - overlap\n",
      "    return sections\n",
      "\n",
      "Attempt 1/10\n",
      "```python\n",
      "def prepare_jobs(\n",
      "    context: List[str],\n",
      "    prev_job_manifests: Optional[List[JobManifest]] = None,\n",
      "    prev_job_outputs: Optional[List[JobOutput]] = None,\n",
      ") -> List[JobManifest]:\n",
      "    job_manifests = []\n",
      "    \n",
      "    # Assuming each document is large, we will chunk it by sections\n",
      "    for doc in context:\n",
      "        chunks = chunk_by_section(doc, max_chunk_size=3000, overlap=20)\n",
      "        \n",
      "        # Task: Extract fasting blood glucose levels\n",
      "        task = \"Extract the fasting blood glucose level and the corresponding date from this chunk.\"\n",
      "        advice = \"Focus on laboratory results or vital signs sections for glucose levels.\"\n",
      "        \n",
      "        for chunk in chunks:\n",
      "            job_manifest = JobManifest(chunk=chunk, task=task, advice=advice)\n",
      "            job_manifests.append(job_manifest)\n",
      "    \n",
      "    return job_manifests\n",
      "\n",
      "def transform_outputs(\n",
      "    jobs: List[Job],\n",
      ") -> str:\n",
      "    glucose_levels = {}\n",
      "    \n",
      "    for job in jobs:\n",
      "        output = job.output\n",
      "        if output.answer:\n",
      "            # Assuming the answer format is \"Date: <date>, Glucose Level: <level>\"\n",
      "            parts = output.answer.split(\", \")\n",
      "            date_part = parts[0].split(\": \")[1]\n",
      "            glucose_level_part = parts[1].split(\": \")[1]\n",
      "            \n",
      "            # Store the glucose level with the date as the key\n",
      "            glucose_levels[date_part] = float(glucose_level_part)\n",
      "    \n",
      "    # Sort the dates to find the admission and Day 5 values\n",
      "    sorted_dates = sorted(glucose_levels.keys())\n",
      "    admission_date = sorted_dates[0]\n",
      "    day_5_date = sorted_dates[4] if len(sorted_dates) > 4 else None\n",
      "    \n",
      "    if day_5_date:\n",
      "        admission_glucose = glucose_levels[admission_date]\n",
      "        day_5_glucose = glucose_levels[day_5_date]\n",
      "        \n",
      "        # Calculate percentage decrease\n",
      "        percentage_decrease = ((admission_glucose - day_5_glucose) / admission_glucose) * 100\n",
      "        result = f\"Percentage decrease in fasting blood glucose from admission to Day 5: {percentage_decrease:.2f}%\"\n",
      "    else:\n",
      "        result = \"Insufficient data to determine the percentage decrease in fasting blood glucose.\"\n",
      "    \n",
      "    return result\n",
      "```\n",
      "Created 2 job manifests (2 chunks, apriori requested 1 samples per chunk, 1 tasks)\n",
      "Total number of job_manifests: 2\n",
      "Sending 2 worker chats to the worker client\n",
      "180 mg/dL on admission\n",
      "Error executing transformation code: IndexError: list index out of range\n",
      "After filtering, 1/1 jobs were included\n",
      "Attempt 1/5 response: {\n",
      "  \"explanation\": \"We have the fasting blood glucose level on admission but lack the Day 5 level, which is necessary to calculate the percentage decrease.\",\n",
      "  \"feedback\": \"Extract the fasting blood glucose level on Day 5.\",\n",
      "  \"decision\": \"request_additional_info\",\n",
      "  \"answer\": null,\n",
      "  \"scratchpad\": \"Collected the admission glucose level: 180 mg/dL. Missing Day 5 glucose level to calculate the percentage decrease.\"\n",
      "}\n",
      "Round 2/5\n",
      "def chunk_by_section(\n",
      "    doc: str, max_chunk_size: int = 3000, overlap: int = 20\n",
      ") -> List[str]:\n",
      "    sections = []\n",
      "    start = 0\n",
      "    while start < len(doc):\n",
      "        end = start + max_chunk_size\n",
      "        sections.append(doc[start:end])\n",
      "        start += max_chunk_size - overlap\n",
      "    return sections\n",
      "\n",
      "Attempt 1/10\n",
      "Here's how you can implement the two functions, `prepare_jobs` and `transform_outputs`, to handle the task of calculating the percentage decrease in fasting blood glucose levels from admission to Day 5 using the patient visit notes:\n",
      "\n",
      "```python\n",
      "def prepare_jobs(\n",
      "    context: List[str],\n",
      "    prev_job_manifests: Optional[List[JobManifest]] = None,\n",
      "    prev_job_outputs: Optional[List[JobOutput]] = None,\n",
      ") -> List[JobManifest]:\n",
      "    job_manifests = []\n",
      "    task_id_1 = 1  # Task ID for extracting admission glucose level\n",
      "    task_id_2 = 2  # Task ID for extracting Day 5 glucose level\n",
      "\n",
      "    # Chunk the documents by section\n",
      "    for doc in context:\n",
      "        chunks = chunk_by_section(doc)\n",
      "\n",
      "        for chunk in chunks:\n",
      "            # Task to extract fasting blood glucose level at admission\n",
      "            task_1 = \"Extract the fasting blood glucose level at admission.\"\n",
      "            job_manifest_1 = JobManifest(\n",
      "                chunk=chunk,\n",
      "                task=task_1,\n",
      "                advice=\"Look for the initial lab results or vital signs section.\",\n",
      "                task_id=task_id_1\n",
      "            )\n",
      "            job_manifests.append(job_manifest_1)\n",
      "\n",
      "            # Task to extract fasting blood glucose level on Day 5\n",
      "            task_2 = \"Extract the fasting blood glucose level on Day 5.\"\n",
      "            job_manifest_2 = JobManifest(\n",
      "                chunk=chunk,\n",
      "                task=task_2,\n",
      "                advice=\"Look for lab results or vital signs section labeled as Day 5.\",\n",
      "                task_id=task_id_2\n",
      "            )\n",
      "            job_manifests.append(job_manifest_2)\n",
      "\n",
      "    return job_manifests\n",
      "\n",
      "def transform_outputs(\n",
      "    jobs: List[Job],\n",
      ") -> str:\n",
      "    def filter_fn(job):\n",
      "        return job.output.answer is not None and job.output.answer.strip() != \"\"\n",
      "\n",
      "    # Filter jobs\n",
      "    for job in jobs:\n",
      "        job.include = filter_fn(job)\n",
      "\n",
      "    # Aggregate and filter jobs\n",
      "    tasks = {}\n",
      "    for job in jobs:\n",
      "        task_id = job.manifest.task_id\n",
      "        chunk_id = job.manifest.chunk_id\n",
      "\n",
      "        if task_id not in tasks:\n",
      "            tasks[task_id] = {\n",
      "                \"task_id\": task_id,\n",
      "                \"task\": job.manifest.task,\n",
      "                \"chunks\": {},\n",
      "            }\n",
      "\n",
      "        if chunk_id not in tasks[task_id][\"chunks\"]:\n",
      "            tasks[task_id][\"chunks\"][chunk_id] = []\n",
      "\n",
      "        tasks[task_id][\"chunks\"][chunk_id].append(job)\n",
      "\n",
      "    # Build the aggregated string\n",
      "    aggregated_str = \"\"\n",
      "    admission_glucose = None\n",
      "    day5_glucose = None\n",
      "\n",
      "    for task_id, task_info in tasks.items():\n",
      "        aggregated_str += f\"## Task (task_id=`{task_id}`): {task_info['task']}\\n\\n\"\n",
      "\n",
      "        for chunk_id, chunk_jobs in task_info[\"chunks\"].items():\n",
      "            filtered_jobs = [j for j in chunk_jobs if j.include]\n",
      "\n",
      "            aggregated_str += f\"### Chunk # {chunk_id}\\n\"\n",
      "            if filtered_jobs:\n",
      "                for idx, job in enumerate(filtered_jobs, start=1):\n",
      "                    aggregated_str += f\"   -- Job {idx} (job_id=`{job.manifest.job_id}`):\\n\"\n",
      "                    aggregated_str += f\"   {job.sample}\\n\"\n",
      "\n",
      "                    # Extract glucose levels\n",
      "                    if task_id == 1 and admission_glucose is None:\n",
      "                        admission_glucose = float(job.output.answer)\n",
      "                    elif task_id == 2 and day5_glucose is None:\n",
      "                        day5_glucose = float(job.output.answer)\n",
      "\n",
      "            else:\n",
      "                aggregated_str += \"   No jobs returned successfully for this chunk.\\n\"\n",
      "\n",
      "        aggregated_str += \"\\n-----------------------\\n\"\n",
      "\n",
      "    # Calculate percentage decrease if both values are found\n",
      "    if admission_glucose is not None and day5_glucose is not None:\n",
      "        percentage_decrease = ((admission_glucose - day5_glucose) / admission_glucose) * 100\n",
      "        aggregated_str += f\"\\nFinal Calculation: The percentage decrease in fasting blood glucose from admission to Day 5 is {percentage_decrease:.2f}%.\\n\"\n",
      "\n",
      "    return aggregated_str\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "- **prepare_jobs**: This function creates tasks to extract fasting blood glucose levels at two specific times: admission and Day 5. It uses chunking to handle large documents and assigns unique task IDs for each extraction task.\n",
      "- **transform_outputs**: This function filters and aggregates the outputs from the small language models. It calculates the percentage decrease in glucose levels if both values are successfully extracted. The final result is included in the aggregated string for review.\n",
      "Created 4 job manifests (2 chunks, apriori requested 1 samples per chunk, 2 tasks)\n",
      "Total number of job_manifests: 4\n",
      "Sending 4 worker chats to the worker client\n",
      "180\n",
      "Attempt 1/5 response: {\n",
      "  \"explanation\": \"We have the admission glucose level but are missing the Day 5 glucose level, which is necessary to calculate the percentage decrease.\",\n",
      "  \"feedback\": \"Extract the fasting blood glucose level on Day 5 from the patient visit notes.\",\n",
      "  \"decision\": \"request_additional_info\",\n",
      "  \"answer\": null,\n",
      "  \"scratchpad\": \"Collected the admission glucose level: 180 mg/dL. Missing Day 5 glucose level to calculate the percentage decrease.\"\n",
      "}\n",
      "Round 3/5\n",
      "def chunk_by_section(\n",
      "    doc: str, max_chunk_size: int = 3000, overlap: int = 20\n",
      ") -> List[str]:\n",
      "    sections = []\n",
      "    start = 0\n",
      "    while start < len(doc):\n",
      "        end = start + max_chunk_size\n",
      "        sections.append(doc[start:end])\n",
      "        start += max_chunk_size - overlap\n",
      "    return sections\n",
      "\n",
      "Attempt 1/10\n",
      "Here's how you can implement the two functions, `prepare_jobs` and `transform_outputs`, based on the requirements and constraints provided:\n",
      "\n",
      "```python\n",
      "def prepare_jobs(\n",
      "    context: List[str],\n",
      "    prev_job_manifests: Optional[List[JobManifest]] = None,\n",
      "    prev_job_outputs: Optional[List[JobOutput]] = None,\n",
      ") -> List[JobManifest]:\n",
      "    job_manifests = []\n",
      "    task_id_1 = 1  # Task ID for extracting fasting blood glucose at admission\n",
      "    task_id_2 = 2  # Task ID for extracting fasting blood glucose on Day 5\n",
      "\n",
      "    # Chunk the documents by section\n",
      "    for doc in context:\n",
      "        chunks = chunk_by_section(doc)\n",
      "\n",
      "        # Create tasks for each chunk\n",
      "        for chunk in chunks:\n",
      "            # Task to extract fasting blood glucose at admission\n",
      "            task_1 = \"Extract the fasting blood glucose level at admission.\"\n",
      "            job_manifest_1 = JobManifest(\n",
      "                chunk=chunk,\n",
      "                task=task_1,\n",
      "                advice=\"Look for the initial lab results or vital signs section.\",\n",
      "                task_id=task_id_1\n",
      "            )\n",
      "            job_manifests.append(job_manifest_1)\n",
      "\n",
      "            # Task to extract fasting blood glucose on Day 5\n",
      "            task_2 = \"Extract the fasting blood glucose level on Day 5.\"\n",
      "            job_manifest_2 = JobManifest(\n",
      "                chunk=chunk,\n",
      "                task=task_2,\n",
      "                advice=\"Look for lab results or vital signs specifically labeled as Day 5.\",\n",
      "                task_id=task_id_2\n",
      "            )\n",
      "            job_manifests.append(job_manifest_2)\n",
      "\n",
      "    return job_manifests\n",
      "\n",
      "def transform_outputs(\n",
      "    jobs: List[Job],\n",
      ") -> str:\n",
      "    def filter_fn(job):\n",
      "        return job.output.answer is not None and job.output.answer.strip() != \"\"\n",
      "\n",
      "    # Filter jobs\n",
      "    for job in jobs:\n",
      "        job.include = filter_fn(job)\n",
      "\n",
      "    # Aggregate and filter jobs\n",
      "    tasks = {}\n",
      "    for job in jobs:\n",
      "        task_id = job.manifest.task_id\n",
      "        chunk_id = job.manifest.chunk_id\n",
      "\n",
      "        if task_id not in tasks:\n",
      "            tasks[task_id] = {\n",
      "                \"task_id\": task_id,\n",
      "                \"task\": job.manifest.task,\n",
      "                \"chunks\": {},\n",
      "            }\n",
      "\n",
      "        if chunk_id not in tasks[task_id][\"chunks\"]:\n",
      "            tasks[task_id][\"chunks\"][chunk_id] = []\n",
      "\n",
      "        tasks[task_id][\"chunks\"][chunk_id].append(job)\n",
      "\n",
      "    # Build the aggregated string\n",
      "    aggregated_str = \"\"\n",
      "    for task_id, task_info in tasks.items():\n",
      "        aggregated_str += f\"## Task (task_id=`{task_id}`): {task_info['task']}\\n\\n\"\n",
      "\n",
      "        for chunk_id, chunk_jobs in task_info[\"chunks\"].items():\n",
      "            filtered_jobs = [j for j in chunk_jobs if j.include]\n",
      "\n",
      "            aggregated_str += f\"### Chunk # {chunk_id}\\n\"\n",
      "            if filtered_jobs:\n",
      "                for idx, job in enumerate(filtered_jobs, start=1):\n",
      "                    aggregated_str += f\"   -- Job {idx} (job_id=`{job.manifest.job_id}`):\\n\"\n",
      "                    aggregated_str += f\"   {job.sample}\\n\\n\"\n",
      "            else:\n",
      "                aggregated_str += \"   No jobs returned successfully for this chunk.\\n\\n\"\n",
      "\n",
      "        aggregated_str += \"\\n-----------------------\\n\\n\"\n",
      "\n",
      "    return aggregated_str\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "\n",
      "- **prepare_jobs**: This function creates tasks to extract fasting blood glucose levels at two different times: admission and Day 5. It uses a chunking strategy to divide the document into manageable sections and assigns a unique task ID for each type of extraction task.\n",
      "\n",
      "- **transform_outputs**: This function filters and aggregates the outputs from the small language models. It filters out jobs with empty or null answers and organizes the results by task and chunk, creating a structured string for review.\n",
      "Created 4 job manifests (2 chunks, apriori requested 1 samples per chunk, 2 tasks)\n",
      "Total number of job_manifests: 4\n",
      "Sending 4 worker chats to the worker client\n",
      "180\n",
      "Attempt 1/5 response: {\n",
      "  \"explanation\": \"We have the admission glucose level but need the Day 5 glucose level to calculate the percentage decrease.\",\n",
      "  \"feedback\": \"Look for the fasting blood glucose level on Day 5.\",\n",
      "  \"decision\": \"request_additional_info\",\n",
      "  \"answer\": null,\n",
      "  \"scratchpad\": \"Collected the admission glucose level: 180 mg/dL. Missing Day 5 glucose level to calculate the percentage decrease.\"\n",
      "}\n",
      "Round 4/5\n",
      "def chunk_by_section(\n",
      "    doc: str, max_chunk_size: int = 3000, overlap: int = 20\n",
      ") -> List[str]:\n",
      "    sections = []\n",
      "    start = 0\n",
      "    while start < len(doc):\n",
      "        end = start + max_chunk_size\n",
      "        sections.append(doc[start:end])\n",
      "        start += max_chunk_size - overlap\n",
      "    return sections\n",
      "\n",
      "Attempt 1/10\n",
      "Here's how you can implement the two functions, `prepare_jobs` and `transform_outputs`, based on the requirements and constraints provided:\n",
      "\n",
      "```python\n",
      "def prepare_jobs(\n",
      "    context: List[str],\n",
      "    prev_job_manifests: Optional[List[JobManifest]] = None,\n",
      "    prev_job_outputs: Optional[List[JobOutput]] = None,\n",
      ") -> List[JobManifest]:\n",
      "    job_manifests = []\n",
      "    task_id = 1  # Unique identifier for the task\n",
      "\n",
      "    # Chunk the context documents by section\n",
      "    for doc in context:\n",
      "        chunks = chunk_by_section(doc)\n",
      "\n",
      "        # Create tasks for each chunk\n",
      "        for chunk in chunks:\n",
      "            # Task to extract fasting blood glucose level at admission\n",
      "            task_admission = \"Extract the fasting blood glucose level at admission.\"\n",
      "            job_manifest_admission = JobManifest(\n",
      "                chunk=chunk,\n",
      "                task=task_admission,\n",
      "                advice=\"Look for the initial blood glucose measurement upon admission.\"\n",
      "            )\n",
      "            job_manifests.append(job_manifest_admission)\n",
      "\n",
      "            # Task to extract fasting blood glucose level on Day 5\n",
      "            task_day5 = \"Extract the fasting blood glucose level on Day 5.\"\n",
      "            job_manifest_day5 = JobManifest(\n",
      "                chunk=chunk,\n",
      "                task=task_day5,\n",
      "                advice=\"Look for the blood glucose measurement specifically on Day 5.\"\n",
      "            )\n",
      "            job_manifests.append(job_manifest_day5)\n",
      "\n",
      "    return job_manifests\n",
      "\n",
      "def transform_outputs(\n",
      "    jobs: List[Job],\n",
      ") -> str:\n",
      "    def filter_fn(job):\n",
      "        answer = job.output.answer\n",
      "        return answer is not None and str(answer).strip().lower() not in [\"none\", \"null\", \"\"]\n",
      "\n",
      "    # Filter jobs\n",
      "    for job in jobs:\n",
      "        job.include = filter_fn(job)\n",
      "\n",
      "    # Aggregate and filter jobs\n",
      "    tasks = {}\n",
      "    for job in jobs:\n",
      "        task_id = job.manifest.task_id\n",
      "        chunk_id = job.manifest.chunk_id\n",
      "\n",
      "        if task_id not in tasks:\n",
      "            tasks[task_id] = {\n",
      "                \"task_id\": task_id,\n",
      "                \"task\": job.manifest.task,\n",
      "                \"chunks\": {},\n",
      "            }\n",
      "\n",
      "        if chunk_id not in tasks[task_id][\"chunks\"]:\n",
      "            tasks[task_id][\"chunks\"][chunk_id] = []\n",
      "\n",
      "        tasks[task_id][\"chunks\"][chunk_id].append(job)\n",
      "\n",
      "    # Build the aggregated string\n",
      "    aggregated_str = \"\"\n",
      "    for task_id, task_info in tasks.items():\n",
      "        aggregated_str += f\"## Task (task_id=`{task_id}`): {task_info['task']}\\n\\n\"\n",
      "\n",
      "        for chunk_id, chunk_jobs in task_info[\"chunks\"].items():\n",
      "            filtered_jobs = [j for j in chunk_jobs if j.include]\n",
      "\n",
      "            aggregated_str += f\"### Chunk # {chunk_id}\\n\"\n",
      "            if filtered_jobs:\n",
      "                for idx, job in enumerate(filtered_jobs, start=1):\n",
      "                    aggregated_str += f\"   -- Job {idx} (job_id=`{job.manifest.job_id}`):\\n\"\n",
      "                    aggregated_str += f\"   {job.sample}\\n\\n\"\n",
      "            else:\n",
      "                aggregated_str += \"   No jobs returned successfully for this chunk.\\n\\n\"\n",
      "\n",
      "        aggregated_str += \"\\n-----------------------\\n\\n\"\n",
      "\n",
      "    return aggregated_str\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "\n",
      "- **prepare_jobs**: This function creates tasks for each chunk of the document. It focuses on extracting the fasting blood glucose levels at two specific times: admission and Day 5. Each task is atomic, focusing on a single extraction point.\n",
      "\n",
      "- **transform_outputs**: This function filters and aggregates the outputs from the small language models. It filters out jobs with no meaningful answers and organizes the results by task and chunk, providing a clear and structured output for review.\n",
      "Created 4 job manifests (2 chunks, apriori requested 1 samples per chunk, 2 tasks)\n",
      "Total number of job_manifests: 4\n",
      "Sending 4 worker chats to the worker client\n",
      "180\n",
      "Attempt 1/5 response: {\n",
      "  \"explanation\": \"We have the admission glucose level but are missing the Day 5 glucose level, which is necessary to calculate the percentage decrease.\",\n",
      "  \"feedback\": \"Look for the fasting blood glucose level on Day 5.\",\n",
      "  \"decision\": \"request_additional_info\",\n",
      "  \"answer\": null,\n",
      "  \"scratchpad\": \"Collected the admission glucose level: 180 mg/dL. Missing Day 5 glucose level to calculate the percentage decrease.\"\n",
      "}\n",
      "Round 5/5\n",
      "def chunk_by_section(\n",
      "    doc: str, max_chunk_size: int = 3000, overlap: int = 20\n",
      ") -> List[str]:\n",
      "    sections = []\n",
      "    start = 0\n",
      "    while start < len(doc):\n",
      "        end = start + max_chunk_size\n",
      "        sections.append(doc[start:end])\n",
      "        start += max_chunk_size - overlap\n",
      "    return sections\n",
      "\n",
      "Attempt 1/10\n",
      "Here's how you can implement the two functions, `prepare_jobs` and `transform_outputs`, based on the requirements and constraints provided:\n",
      "\n",
      "```python\n",
      "def prepare_jobs(\n",
      "    context: List[str],\n",
      "    prev_job_manifests: Optional[List[JobManifest]] = None,\n",
      "    prev_job_outputs: Optional[List[JobOutput]] = None,\n",
      ") -> List[JobManifest]:\n",
      "    job_manifests = []\n",
      "    task_id = 1  # Unique identifier for the task\n",
      "\n",
      "    # Chunk the documents by section\n",
      "    for doc in context:\n",
      "        chunks = chunk_by_section(doc, max_chunk_size=3000, overlap=20)\n",
      "        \n",
      "        # Create tasks for each chunk\n",
      "        for chunk in chunks:\n",
      "            # Task to extract fasting blood glucose level at admission\n",
      "            task_admission = \"Extract the fasting blood glucose level at admission.\"\n",
      "            job_manifest_admission = JobManifest(\n",
      "                chunk=chunk,\n",
      "                task=task_admission,\n",
      "                advice=\"Look for the initial blood glucose measurement upon admission.\"\n",
      "            )\n",
      "            job_manifests.append(job_manifest_admission)\n",
      "\n",
      "            # Task to extract fasting blood glucose level on Day 5\n",
      "            task_day5 = \"Extract the fasting blood glucose level on Day 5.\"\n",
      "            job_manifest_day5 = JobManifest(\n",
      "                chunk=chunk,\n",
      "                task=task_day5,\n",
      "                advice=\"Look for the blood glucose measurement recorded on Day 5.\"\n",
      "            )\n",
      "            job_manifests.append(job_manifest_day5)\n",
      "\n",
      "    return job_manifests\n",
      "\n",
      "def transform_outputs(\n",
      "    jobs: List[Job],\n",
      ") -> str:\n",
      "    def filter_fn(job):\n",
      "        answer = job.output.answer\n",
      "        return answer is not None and str(answer).strip().lower() not in [\"none\", \"null\", \"\"]\n",
      "\n",
      "    # Filter jobs\n",
      "    for job in jobs:\n",
      "        job.include = filter_fn(job)\n",
      "\n",
      "    # Aggregate and filter jobs\n",
      "    tasks = {}\n",
      "    for job in jobs:\n",
      "        task_id = job.manifest.task_id\n",
      "        chunk_id = job.manifest.chunk_id\n",
      "\n",
      "        if task_id not in tasks:\n",
      "            tasks[task_id] = {\n",
      "                \"task_id\": task_id,\n",
      "                \"task\": job.manifest.task,\n",
      "                \"chunks\": {},\n",
      "            }\n",
      "\n",
      "        if chunk_id not in tasks[task_id][\"chunks\"]:\n",
      "            tasks[task_id][\"chunks\"][chunk_id] = []\n",
      "\n",
      "        tasks[task_id][\"chunks\"][chunk_id].append(job)\n",
      "\n",
      "    # Build the aggregated string\n",
      "    aggregated_str = \"\"\n",
      "    for task_id, task_info in tasks.items():\n",
      "        aggregated_str += f\"## Task (task_id=`{task_id}`): {task_info['task']}\\n\\n\"\n",
      "\n",
      "        for chunk_id, chunk_jobs in task_info[\"chunks\"].items():\n",
      "            filtered_jobs = [j for j in chunk_jobs if j.include]\n",
      "\n",
      "            aggregated_str += f\"### Chunk # {chunk_id}\\n\"\n",
      "            if filtered_jobs:\n",
      "                for idx, job in enumerate(filtered_jobs, start=1):\n",
      "                    aggregated_str += f\"   -- Job {idx} (job_id=`{job.manifest.job_id}`):\\n\"\n",
      "                    aggregated_str += f\"   {job.sample}\\n\\n\"\n",
      "            else:\n",
      "                aggregated_str += \"   No jobs returned successfully for this chunk.\\n\\n\"\n",
      "\n",
      "        aggregated_str += \"\\n-----------------------\\n\\n\"\n",
      "\n",
      "    return aggregated_str\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "\n",
      "- **prepare_jobs**: This function creates tasks for each chunk of the document. It generates two tasks per chunk: one to extract the fasting blood glucose level at admission and another for Day 5. Each task is atomic, focusing on a single piece of information.\n",
      "\n",
      "- **transform_outputs**: This function filters and aggregates the outputs from the small language models. It uses a filter function to exclude jobs with empty or null answers. The results are then organized by task and chunk, and an aggregated string is constructed for review.\n",
      "Created 4 job manifests (2 chunks, apriori requested 1 samples per chunk, 2 tasks)\n",
      "Total number of job_manifests: 4\n",
      "Sending 4 worker chats to the worker client\n",
      "180 mg/dL\n",
      "Attempt 1/5 response: {\n",
      "  \"explanation\": \"The fasting blood glucose level at admission was collected as 180 mg/dL. However, the Day 5 glucose level is missing, which is necessary to calculate the percentage decrease.\",\n",
      "  \"feedback\": \"Missing Day 5 glucose level to complete the calculation.\",\n",
      "  \"decision\": \"provide_final_answer\",\n",
      "  \"answer\": null,\n",
      "  \"scratchpad\": \"Collected the admission glucose level: 180 mg/dL. Missing Day 5 glucose level to calculate the percentage decrease.\"\n",
      "}\n",
      "Exhausted all rounds without finding a final answer. Returning the last synthesized response.\n",
      "\n",
      "=== SAVED LOG TO minions_logs/20250715_084536_Using_the_elect_minions.json ===\n",
      "\n",
      "=== MINIONS TASK COMPLETED ===\n"
     ]
    }
   ],
   "source": [
    "output = protocol(\n",
    "        task=task,\n",
    "        doc_metadata=doc_metadata,\n",
    "        context=[context],\n",
    "        max_rounds=5,  # you can adjust rounds as needed for testing\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
